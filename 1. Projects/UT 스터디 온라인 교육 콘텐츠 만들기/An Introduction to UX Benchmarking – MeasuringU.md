[[ReadItLater]] [[Article]]

# [An Introduction to UX Benchmarking – MeasuringU](https://measuringu.com/benchmark-intro/)

![](Attachment/benchmark-clocks.jpg)

UX benchmarking is an effective method for understanding how people use and think about an interface, whether it’s for a website, software, or mobile app.

Benchmarking becomes an essential part of a [plan to systematically improve](https://measuringu.com/ux-measurement/) the user experience.

A lot is involved in conducting an effective benchmark. I’m covering many of the details in a 4-week course sponsored by the [UXPA this October 2017](https://measuringu.com/events/benchmarking-the-user-experience/). More on benchmarking will be available in my forthcoming book, [Benchmarking the User Experience](https://measuringu.com/book/benchmarking-the-user-experience/).

To start, benchmarking the user experience effectively means first understanding both what benchmarking is, what the user experience is, and then progressing to methods, metrics, and analysis.

## What Is User Experience?

Few things seem to elicit more disagreement than the definition of user experience and how it may or may not differ from user interface design or usability testing. While I don’t intend to offer an official definition (there’s some health in the debate), here’s the definition I use similar to [Tullis & Albert](http://amzn.to/2yTWga1): The user experience is the combination of all the behaviors and attitudes people have while interacting with an interface. These include but aren’t limited to:

-   Ability to complete tasks
-   The time it takes to complete tasks or find information
-   Ability to find products or information
-   Attitudes toward visual appearance
-   Attitudes toward trust and credibility
-   Perceptions of ease, usefulness and satisfaction

These are also many of the classic [usability testing metrics](https://measuringu.com/essential-metrics/), but include broader metrics dealing with attitudes, branding, loyalty, and appearance. As such, we borrow heavily from usability testing methods and terminology.

### What Is Benchmarking and Why Do It?

A benchmark is a standard or point of reference against which metrics may be compared or assessed. This provides a good idea around its purpose. It has an interesting etymology: It originally comes from land surveyors who would cut a mark into a stone to secure a bracket called a “bench.” This would be used as a point of reference for building.

With computers, a benchmark is usually an evaluation that assesses the performance of software or hardware to set standards for future tests or trials to gauge performance against (such as CPU or database performance). Similarly, UX benchmarking involves evaluating an interface using a standard set of metrics to gauge its relative performance.

![](https://measuringu.com/wp-content/uploads/2017/10/benchmark-cut.jpg "Benchmark_(surveying)")

**Figure 1:** A mark used by surveyors to place the “bench” or leveling-rod for setting the correct elevation.

One of the hallmarks of measuring the user experience is seeing whether design efforts actually made a quantifiable difference over time. A regular benchmark study is a great way to institutionalize that. Benchmarks are most effective when done on a regular interval (e.g. every year or quarter) or after significant design or feature changes. Figure 2 shows the same tasks from an automotive website across three years: 2011, 2012, and 2014.

![](Attachment/yoy-benchmark.jpg)

**Figure 2:** Completion rates for five tasks from a UX benchmarking study for an automotive website.

A good benchmark indicates where a website or product falls relative to some meaningful comparison. It can be compared to:

-   Earlier versions of the product/website
-   The competition
-   Relative to an industry
-   An industry standard (such as an NPS or conversation rate)
-   Other products in the same company

## What Can You Benchmark?

While just about anything can be benchmarked, the most common interfaces that have benchmark evaluations are:

-   Websites (B2C & B2B): The shopping experience on Walmart.com, Costco, or GE.com
-   Desktop software (B2C & B2B): QuickBooks, Excel, or iTunes
-   Web apps: Salesforce.com or MailChimp
-   Mobile websites: PayPal’s mobile website
-   Mobile apps: Facebook, Snapchat, or a Chase mobile banking app
-   Physical devices: Remote controls, in-car entertainment systems, or medical devices
-   Internal apps within a company: Expense reporting applications or HR systems
-   Service experiences: Customer support calls or out-of-the-box experiences (OOBE)

## Two Types of Benchmarking Studies

There are essentially two types of benchmark studies: retrospective and task-based.

**Retrospective**: Participants are asked to recall their most recent experience with an interface and answer questions. We used this approach for the [Consumer Software](https://measuringu.com/product/consumer-software2017/) and [Business Software](https://measuringu.com/product/business-software-2017/) Benchmark reports. With this approach, you don’t need access to the software but are limited to testing with only existing users who need to recall past actions (which can be fallible).

**Task-Based**: Participants are asked to attempt prescribed tasks on the interface, simulating actual usage in a controlled setting (sometimes also called a concurrent study). This is a common usability test setup and is what we use most often when [working with clients](https://measuringu.com/services/competitive-benchmark/). With this approach, you get more detailed task-interactions and can test with new and existing users, but need access to the software or apps and need to [define tasks](https://measuringu.com/task-tips/) and [success criteria](https://measuringu.com/determine-task-completion/).

Retrospective and task-based studies focus on different experiences as shown in Figure 3.  
![](Attachment/benchmark-types.jpg)

**Figure 3:** Retrospective and task-based benchmark studies focus on different parts of the experience: existing attitudes about prior use (retrospective) and current attitudes and actions from interacting through simulated use (task-based).

UX benchmark studies fortunately can use a mix of retrospective and task-based studies. We take this approach whenever we can. We start by asking current customers to reflect on their experiences, and then ask a mix of new and existing customers attempt to complete tasks.

We used the mixed-approach with our [Hotel UX benchmark study](https://measuringu.com/product/hotels-2017/). Four hundred and five participants reflected on one of five hotel websites and answered a set of questions about the experience including the [SUPR-Q](https://measuringu.com/product/suprq/) (retrospective). Another 160 participants who had booked ANY hotel online were randomly assigned to complete two tasks on the same hotel websites (task-based or concurrent). This gave us a fuller picture of the user experience than if we had used only one type of study.

## Different Modes of UX Benchmarking

When conducting a task-based UX benchmark, you need to choose between the different types of modes: moderated or unmoderated testing.

**Moderated testing**: Moderated testing requires a facilitator/moderator with the participant. Moderated testing can be conducted in-person or remotely using monitoring software such as GoToMeeting or WebEx.

**Unmoderated testing**: Unmoderated testing is similar to a survey. Participants essentially self-administer the study by following directions to answer questions and attempt tasks. Software, such as our [MUIQ platform](https://measuringu.com/services/muiq/), Loop11, or UserZoom, help automate the process and collect a rich set of data including timing, clicks, heat maps, and videos. There are also some low-cost lower-tech solutions, such as using a survey platform like SurveyMonkey, to have participants complete tasks and then reflect on the experience (but at the cost of no automatically collected metrics and no videos).

While there are many advantages and disadvantages to [moderated or unmoderated testing](https://measuringu.com/method-comparison/), the major difference is that unmoderated testing allows you to collect data from more participants in more locations quickly. You sacrifice the richness of a one- to-one interaction but often for many benchmarking studies, it’s worth sacrificing to get larger numbers.

## Benchmark Metrics

Benchmarks are all about the metrics they collect. Benchmark studies are often called [summative evaluations](https://measuringu.com/formative-summative/) where the emphasis is less on finding problems but on assessing the current experience. That experience is quantified using both broader study-level metrics and granular task-level metrics (if there are tasks).

### Study-Based Metrics

These metrics are typically collected at the end and or beginning of a study (either task-based or retrospective).

**SUPR-Q**: Provides [a measure](https://measuringu.com/product/suprq/) of the overall quality of the website user experience plus measures of usability, appearance, trust, and loyalty.

**SUPR-Qm**: A questionnaire for the mobile app user experience (in press).

**SUS**: A measure of [perceived usability](https://measuringu.com/10-things-sus/); good for software.

**NPS**: A measure of [customer loyalty](https://measuringu.com/ux-changes-nps/) for all interfaces; better for consumer-facing ones.

**UMUX-Lite**: A compact measure of perceived usefulness and perceived ease (article forthcoming).

**Brand attitude/brand lift**: [Brand](https://measuringu.com/control-brand/) has a significant effect on UX metrics. Measuring brand before and after a study helps identify how much the experience has (positive or negative) on brand attitudes.

### Task-Level Metrics

For studies with tasks, the following are the most common metrics collected as part of the task, or after the task.

**Attitudes**: Perceptions of Ease ([SEQ](https://measuringu.com/seq10/)) and [confidence](https://measuringu.com/measuring-confidence/) (collected after the task)

**Actions**: [Completion rates](https://measuringu.com/completion-rates/), [task times](https://measuringu.com/task-times/), [errors](https://measuringu.com/errors-ux/) (collected from the task)

## Summary

A UX benchmark provides a quantifiable measure of an interface such as websites, mobile apps, products, or software. A good benchmark indicates how the performance of the interface scores relative to a meaningful comparison from an earlier point in time, competition, or industry standard. Benchmarks can be retrospective (participants reflect on actual usage) or task-based (participants attempt tasks in simulated use). Collecting data for UX benchmarking involves the same modes as usability testing: moderated or unmoderated approaches. Benchmarking data should come at the study level (SUPR-Q, SUS, NPS) and if there are tasks, at the task level (completion rates, time, errors, SEQ).



---
# 번역

## 두 가지 유형의 벤치마킹 연구

  

벤치마킹 연구에는 기본적으로 후향적 연구와 작업 기반 연구의 두 가지 유형이 있습니다.

  

**회고적**: 참가자에게 인터페이스에 대한 가장 최근의 경험을 회상하고 질문에 답하도록 요청합니다. 소비자 소프트웨어](https://measuringu.com/product/consumer-software2017/) 및 [비즈니스 소프트웨어](https://measuringu.com/product/business-software-2017/) 벤치마크 보고서에는 이 접근 방식을 사용했습니다. 이 접근법을 사용하면 소프트웨어에 액세스할 필요는 없지만 과거 행동을 기억해야 하는 기존 사용자(오류 가능성이 있음)만을 대상으로 테스트할 수 있다는 한계가 있습니다.

  

**작업 기반**: 참가자는 인터페이스에서 규정된 작업을 시도하도록 요청받으며 통제된 환경에서 실제 사용법을 시뮬레이션합니다(동시 연구라고도 함). 이는 일반적인 사용성 테스트 설정이며 [고객과 함께 작업]할 때 가장 자주 사용하는 방식입니다(https://measuringu.com/services/competitive-benchmark/). 이 접근 방식을 사용하면 보다 상세한 작업 상호 작용을 얻을 수 있고 신규 및 기존 사용자를 대상으로 테스트할 수 있지만 소프트웨어 또는 앱에 액세스해야 하며 [작업 정의](https://measuringu.com/task-tips/) 및 [성공 기준](https://measuringu.com/determine-task-completion/)이 필요합니다.

  

후향적 연구와 작업 기반 연구는 그림 3과 같이 서로 다른 경험에 초점을 맞춥니다.  

![](첨부파일/벤치마크-유형.jpg)

  

**그림 3:** 회고적 연구와 과제 기반 벤치마크 연구는 경험의 다른 부분, 즉 이전 사용에 대한 기존 태도(회고적)와 시뮬레이션 사용을 통한 상호작용을 통한 현재 태도 및 행동(과제 기반)에 초점을 맞추고 있습니다.

  

다행히도 UX 벤치마크 연구는 회고적 연구와 작업 기반 연구를 혼합하여 사용할 수 있습니다. 저희는 가능할 때마다 이 접근법을 사용합니다. 먼저 현재 고객에게 자신의 경험을 되돌아보게 한 다음, 신규 고객과 기존 고객을 혼합하여 과제를 완료하도록 요청합니다.

  

호텔 UX 벤치마크 연구](https://measuringu.com/product/hotels-2017/)에서도 이러한 혼합 접근법을 사용했습니다. 4백 5명의 참가자가 5개의 호텔 웹사이트 중 한 곳을 방문하여 [SUPR-Q](https://measuringu.com/product/suprq/)를 포함한 일련의 질문에 답했습니다(회고). 온라인으로 호텔을 예약한 적이 있는 또 다른 160명의 참가자는 무작위로 배정되어 동일한 호텔 웹사이트에서 두 가지 과제(과제 기반 또는 동시 과제)를 완료했습니다. 이를 통해 한 가지 유형의 연구만 수행했을 때보다 사용자 경험을 더 자세히 파악할 수 있었습니다.



## 다양한 UX 벤치마킹 모드

작업 기반 UX 벤치마크를 수행할 때는 중재 테스트 또는 중재되지 않은 테스트 등 다양한 유형의 모드 중에서 선택해야 합니다.

**중재 테스트**: 중재 테스트에는 참가자와 함께 진행자/중재자가 필요합니다. 중재 테스트는 직접 대면하거나 GoToMeeting 또는 WebEx와 같은 모니터링 소프트웨어를 사용하여 원격으로 진행할 수 있습니다.

**비조정 테스트**: 비조정 테스트는 설문조사와 유사합니다. 참가자는 기본적으로 지시에 따라 질문에 답하고 과제를 시도하는 방식으로 연구를 스스로 관리합니다. MUIQ 플랫폼](https://measuringu.com/services/muiq/), Loop11 또는 UserZoom과 같은 소프트웨어는 프로세스를 자동화하고 타이밍, 클릭 수, 히트 맵, 동영상 등 다양한 데이터를 수집하는 데 도움이 됩니다. 설문조사 플랫폼인 SurveyMonkey를 사용하여 참가자가 과제를 완료한 후 경험을 반영하도록 하는 등 저비용의 저기술 솔루션도 있습니다(단, 자동으로 수집되는 측정지표와 동영상이 없다는 단점이 있습니다).

중재 또는 비중재 테스트](https://measuringu.com/method-comparison/)에는 많은 장단점이 있지만, 가장 큰 차이점은 비중재 테스트를 통해 더 많은 장소에서 더 많은 참가자로부터 데이터를 빠르게 수집할 수 있다는 것입니다. 일대일 상호작용의 풍부함을 희생해야 하지만, 많은 벤치마킹 연구의 경우 더 많은 인원을 확보하기 위해 희생할 가치가 있는 경우가 많습니다.