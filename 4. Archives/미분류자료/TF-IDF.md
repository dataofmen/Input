TF-IDF

## TF-IDF

* TF-IDF는 단어빈도와 역문서 빈도의 곱이다. 처음 접할 때는 -가 있어서 둘이 빼는 건가 했는데 그게 아니었다.
*  문서 내에서 단어의 출현 빈도가 높고 전체 문서에서 단어가 출현하는 문서들의 수가 적은 단어가 중요한 단어로 평가된다.
* 따라서 이 값을 이용하면 흔한 단어를 걸러 낼 수 있다.
*  가령, ‘상승’이라는 단어의 빈도는 매우 높아 불용어 수준이지만 포함하는 문서 수가 적으면 문서내에서 많은 빈도로 언급되고 있다는 의미로서 핵심적인 메시지를 담고 있는 문서로 볼 수 있다.

* TF-IDF는 ’특정 단어의 중요도는 단어가 출현한 횟수에 비례하고 그 단어가 언급된 모든 문서의 총수에 빈비례한다‘는 명제에 기초하고 있다(이말례·배환국, 2002).
* 여러 문서에서 자주 등장하는 단어는 중요하지 않을 확률이 높다는 가정을 충실히 반영하고 있다.


### TF (Term Frequency, 단어 빈도)
TF는 단어빈도로, 특정 단어가 문서내에 얼만큼의 빈도로 등장하는지를 나타낸다. 당연하게도 문서에 자주 등장하는 엔티티라면 엔티티로서의 값어치가 높다고 예상할 수 있다. 매우 직관적으로 이해할 수 있는 관계다.

### DF (Document Frequency, 문서 빈도)
DF는 문서 빈도인데 자주 등장하는 단어가 몇 개의 문서에 등장하는지를 나타낸다. 

### IDF (Inverse Document Frequency)
IDF는 문서 빈도의 역수다. 전체 문서 갯수를 해당 엔티티가 포함된 문서의 갯수로 나누는 것이다. 이것은 해당 문서군의 특징을 보여주는 척도가 될 수 있다. 어떤 문서 내에 클라라라는 단어가 자주 등장했다고 생각해보자. 아마 이 문서군은 연애 기사들의 모음일 가능성이 충분히 있을 것이다. 따라서 이 값을 통하여 해당 문서군에서 가치 있는 엔티티를 찾아낼 수 있는데, 만약 해당 문서군에서 드물게 등장하는 단어가 무엇인지 알수 있다면, 아마 그 단어가 어떤 그 문서군에서 특정 한 문서를 유니크 할 수 있도록 해주고, 눈에 띄게해주는 단어일 확률이 높기 때문이다.


### TF-IDF 계산법 예시
* Term frequency는 어떤 문서 내에서 특정 단어가 나타나는 횟수를 문서에 들어있는 모든 단어 수로 나눈 값입니다. 만약 100개의 단어로 이루어진 어떤 문서에 단어 cow가 세 번 등장한다면, 단어 cow의 term frequency는 0.03(3/100)입니다.

* Document frequency를 계산하는 한 가지 방법은 단어 cow가 포함된 문서들의 숫자를 전체 문서의 숫자로 나누는 방법입니다. 만약, 전체 10,000,000개의 문서 중에서 단어 cow가 들어있는 문서들의 숫자가 1,000개라면, document frequency는 0.0001(1000/10,000,000)입니다.
최종 tf-idf 가중치는 term frequency를 document frequency로 나눠서 구할 수 있습니다. 예를  들어, 단어 cow의 tf-idf 가중치는 300(0.03/0.0001)입니다.

* 또 다른 방법은 document frequency에 로그(일반적으로 자연 로그)를 취하는 방법입니다. 이 방법을 사용하면 idf 값은 ln(10,000,000/1,000) = 9.21이 되어서 tf-idf 가중치는 0.27(0.03 * 9.21)이 됩니다.


### TF-IDF 적용을 위한 변형 절차
TF-IDF는 수식으로만 보면 간단해 보인다. 하지만 정확한 가중치를 계산하기 위해서는 여러 변형 절차를 거쳐야 하는 것이 일반적이다.

#### 정규화(normalization)
*  문서 길이에 따라 가중치를 달리 적용해야 하는 문제부터 해결할 필요가 있다.
* A와 B라는 문서에서 ’블로터‘라는 키워드가 각각 10번과 15번 등장했다고 하자. B문서는 A문서보다 1.5배나 더 길다. 문서가 길어지면 자연스럽게 특정 단어가 출현할 빈도가 높아지기 마련이다. 단어 출현 빈도(TF)라는 측면에서 블로터라는 단어가 어느 문서에서 더 비중있게 다뤄지고 있을까.
* 이를 측정하기 위해서는 정규화(normalization)라는 과정이 진행돼야 한다. 이를테면, A라는 문서가 총 150개 단어로, B 문서는 250개 단어로 이뤄졌다고 하면, ‘블로터’의 TF를 정규화한 값은 A에선 10/150=0.066, B에선 15/250=0.6이 된다. 블로터라는 단어가 B 문서보다 A에서 상대적으로 더 자주 등장한 셈이 된다.

#### 불용어 처리
* TF-IDF를 측정하다 보면 중요하지 않지만 출현 빈도가 높은 단어들이 여럿 존재한다. 기자라는 직책이나 기자명, 언론사명이 여기에 해당한다. 모든 기사엔 기자라는 직책이 포함돼있거나 언론사 명이 표기된다. 혹은 저작권 표시 문구도 어김없이 등장한다. TF는 높지만 IDF는 1로 수렴되는 단어들이다.
* 알고리즘 설계자는 분석에 방해가 되는 이런 류의 단어들을 불용어로 처리한다(이성직 & 김한준, 2009). 분석 대상에서 제외한다는 뜻이다. 


### TF-IDF 활용
* 기계학습 알고리즘을 통해 문서 내 이슈 키워드를 추출해내는 것도 가능하다. 
* TF-IDF는 문서 분류에도 적용이 가능하다. 예를 들어 블로터라는 단어의 TF-IDF 가중치가 특정 값 이상인 문서를 유사 문서로 분류한다고 가정하면 간단한 분류 공식이 성립될 수 있다.
* 특정 상품에 대한 평이 과도하게 부정적이거나 긍정적인 문서를 분류해냄으로써 편향된 리뷰를 걸러낼 수 있는 시스템도 만들어낼 수 있다(Yeon, J., et all, 2013). 물론 다른 알고리즘과의 결합, 변형을 통해서 달성할 수 있는 용도다.


### 참고문헌
* 이말례, & 배환국. (2002). TFIDF 를 이용한 키워드 추출 시스템 설계. 인지과학, 13(1), 1-11.
* 이성직, & 김한준. (2009). TF-IDF 의 변형을 이용한 전자뉴스에서의 키워드 추출 기법. 한국전자거래학회지, 14(4), 59-73.
* Luhn, H. P. (1957). A statistical approach to mechanized encoding and searching of literary information. IBM Journal of research and development, 1(4), 309-317.
* Wu, H. C., Luk, R. W. P., Wong, K. F., & Kwok, K. L. (2008). Interpreting tf-idf term weights as making relevance decisions. ACM Transactions on Information Systems (TOIS), 26(3), 13.
* Yeon, J., Shim, J., & goo Lee, S. (2013). Outlier detection techniques for biased opinion discovery. Journal of Society for e-Business Studies, 18(4).
* http://dev.youngkyu.kr/25
* http://jeongsw.tistory.com/449